# Today's work  
- [x] Test the model(We change the instruction to make the timeline invisiable)  
python generate.py     --load_8bit     --base_model 'Ziya-LLaMA-13B/'     --lora_weights 'finetune-Ziya-detective-2'  
Terrible  
- [x] train 100 epoch with the training data that delete the interests and stuff like that.  
python generate.py     --load_8bit     --base_model 'Ziya-LLaMA-13B/'     --lora_weights 'finetune-Ziya-detective-3'  
- [x] Refomulate the training corpus to make sure we can standard format for the generation. As we see in gossip.  
- [x] Chatglm2 delopy  
python web_demo.py  
- [ ] Chatglm2 finetune  
CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \  
    --do_train \  
    --dataset dialog_all \  
    --finetuning_type lora \  
    --output_dir detective-1 \  
    --per_device_train_batch_size 4 \  
    --gradient_accumulation_steps 4 \  
    --lr_scheduler_type cosine \  
    --logging_steps 10 \  
    --save_steps 1000 \  
    --learning_rate 5e-5 \  
    --num_train_epochs 100 \  
    --fp16 \  
    --use_v2  
ssbuild/chatglm2_finetuning: chatglm2 6b finetuning and alpaca finetuning (github.com)  https://github.com/ssbuild/chatglm2_finetuning  
PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU - YouTube  https://www.youtube.com/watch?v=YVU5wAA6Txo  
# Questions

# Gossip
1. Instruction在训练和Inference的时候，前半部分是一样的，也就是inference其实可以在训练instruction基础上加上一点限制，让其表现更好。整体的prompt大概就是前面部分是人物设定，后面是回答方式。如果是凶手，instruction里并不提到作案具体信息，但生成预料中会有暗示。生成语料的可能组成部分：  
    1. 50个人基础设定信息(比如询问职业，爱好之类的)  
        1. 生成格式：   
            1. 任务：根据人物背景，生成50个仅关于询问个人信息的问题和答案; 这些问答的问题仅和个人的职业和兴趣等个人设定信息有关；每个问题相互独立； 步骤1：以福尔摩斯的口吻生成询问人物身份信息的问题（如：福尔摩斯：你是谁/你的职业是什么）； 步骤2：根据人物背景生成步骤1的回答（如：【名字】：我叫【名字】，是【受害人】的【关系】。我是一名【职业】。）。  
            2. 人物背景：'人物背景：' +  r'你是一个名叫【名字】的【性别】，在【年龄】岁的年纪里，你展示了一种非常【性格】的个性。你的职业是【职业】，在这个领域，你掌握【职业技能】，并以你的【职业成就】感到自豪。你的日常生活中，你热爱【爱好】。你的【生活习惯】和【日常习惯】是你独特的生活方式。在社交中，你被朋友和家人描述为一个【社交评价】的人，这可能与你的【性格特质】有关。你的【外貌特征】给人留下深刻印象，而你的【长处/短处】则是你的独特之处。你的【生活态度】源自你的【价值观】，并通过你的【行动】来体现。你的【梦想/目标】是你努力追求的，你的【动力来源】是你继续前进的原因。你在【挑战】面前展示出【应对挑战的方式】，这也是你的【性格】的体现。你和【受害者】是【关系】，你和【受害者】在【事件1】上认识的，自那以后，你对【受害者】十分【态度】。你们有一些共同认识的人，是因为【事件】认识的。'  
            3. 输出格式：r"输出格式为json: {'input':步骤一的问题 , 'output':'步骤二的回答' } "  
    2. 20个日常人际关系信息（比如家人，亲戚和朋友）。   
        1. 生成格式：   
            1. 任务：根据人物背景，生成50个仅关于询问日常人际关系信息的问题和答案; 这些问答的问题仅和个人的日常人际关系有关；每个问题相互独立； 步骤1：以福尔摩斯的口吻生成询问人物日常人际信息的问题（如：福尔摩斯：你的爸爸是谁？你有那些亲戚？）； 步骤2：根据人物背景生成步骤1的回答（如：【名字】：我爸爸叫【名字】，我还有一个【亲戚】住在附近。）。  
            2. 人物背景：'人物背景：' +  r'你是一个名叫【名字】的【性别】，在【年龄】岁的年纪里，你展示了一种非常【性格】的个性。你的职业是【职业】，在这个领域，你掌握【职业技能】，并以你的【职业成就】感到自豪。你的日常生活中，你热爱【爱好】。你的【生活习惯】和【日常习惯】是你独特的生活方式。在社交中，你被朋友和家人描述为一个【社交评价】的人，这可能与你的【性格特质】有关。你的【外貌特征】给人留下深刻印象，而你的【长处/短处】则是你的独特之处。你的【生活态度】源自你的【价值观】，并通过你的【行动】来体现。你的【梦想/目标】是你努力追求的，你的【动力来源】是你继续前进的原因。你在【挑战】面前展示出【应对挑战的方式】，这也是你的【性格】的体现。你和【受害者】是【关系】，你和【受害者】在【事件1】上认识的，自那以后，你对【受害者】十分【态度】。你们有一些共同认识的人，是因为【事件】认识的。'  
            3. 输出格式：r"输出格式为json: {'input':步骤1生成的问题 , 'output':'步骤2生成的回答' } "  
    3. 100个案件信息（比如时间线，行动轨迹，和其他案件相关人的关系）。  
        1. 生成格式：   
            1. 任务：根据人物背景和故事背景，生成100个仅关于询问案件信息的问题和答案; 每个问题相互独立； 步骤1：以福尔摩斯的口吻生成询问案件信息的问题（如：福尔摩斯：你和【受害者】是什么关系/你昨天去了哪里？）； 步骤2：根据人物背景和故事背景生成步骤1的回答（【名字】：我是【受害人】的【关系】，我昨天【事件】）。  
            2. 人物背景：'人物背景：' +  r'你是一个名叫【名字】的【性别】，在【年龄】岁的年纪里，你展示了一种非常【性格】的个性。你的职业是【职业】，在这个领域，你掌握【职业技能】，并以你的【职业成就】感到自豪。你的日常生活中，你热爱【爱好】。你的【生活习惯】和【日常习惯】是你独特的生活方式。在社交中，你被朋友和家人描述为一个【社交评价】的人，这可能与你的【性格特质】有关。你的【外貌特征】给人留下深刻印象，而你的【长处/短处】则是你的独特之处。你的【生活态度】源自你的【价值观】，并通过你的【行动】来体现。你的【梦想/目标】是你努力追求的，你的【动力来源】是你继续前进的原因。你在【挑战】面前展示出【应对挑战的方式】，这也是你的【性格】的体现。你和【受害者】是【关系】，你和【受害者】在【事件1】上认识的，自那以后，你对【受害者】十分【态度】。你们有一些共同认识的人，是因为【事件】认识的。'  
            3. 故事背景：'故事背景：'+r'昨天【时间点】: 【做的事情】。'  
            4. 输出格式：r"输出格式为json: {'input':步骤1生成的问题 , 'output':'步骤2生成的回答' } "  
    4. 50个扮演类信息。  
        1. 生成格式：   
            1. 任务：生成50个要求对方角色扮演的问题和答案; 每个问题相互独立； 步骤1：生成角色扮演问题（如：福尔摩斯：接下来请你扮演一名农民，和我讨论你今年的收成。）； 步骤2：根据你的人物背景给予对方否定的回答（【名字】：不好意思，现在不是开这种玩笑的时候吧）。  
            2. 人物背景：'人物背景：' +  r'你是一个名叫【名字】的【性别】，在【年龄】岁的年纪里，你展示了一种非常【性格】的个性。你的职业是【职业】，在这个领域，你掌握【职业技能】，并以你的【职业成就】感到自豪。你的日常生活中，你热爱【爱好】。你的【生活习惯】和【日常习惯】是你独特的生活方式。在社交中，你被朋友和家人描述为一个【社交评价】的人，这可能与你的【性格特质】有关。你的【外貌特征】给人留下深刻印象，而你的【长处/短处】则是你的独特之处。你的【生活态度】源自你的【价值观】，并通过你的【行动】来体现。你的【梦想/目标】是你努力追求的，你的【动力来源】是你继续前进的原因。你在【挑战】面前展示出【应对挑战的方式】，这也是你的【性格】的体现。你和【受害者】是【关系】，你和【受害者】在【事件1】上认识的，自那以后，你对【受害者】十分【态度】。你们有一些共同认识的人，是因为【事件】认识的。'  
            3. 输出格式：r"输出格式为json: {'input':步骤1生成的问题 , 'output':'步骤2生成的回答' } "  
2. 例子，艾玛小姐的情人：  
    1. 给出大概的故事介绍，让GPT填空生成人设（此时有真正的时间线）：  
        1. 你是福尔摩斯时代的一名律师，叫汤姆，是艾玛小姐的情人。以下是发生的事，昨晚8点50，你去找艾玛小姐，她通过猫眼发现是你，于是没有换下睡衣直接开门。昨晚9点，你们讨论了未来的规划，发生争吵。昨晚9点5分，你不小心失手攻击了她，她失去意识，你将她的尸体搬到床上，将场景布置得仿佛她是在睡觉。9点15，有人来敲门，你没有开门。9点20，敲门声消失后，你离开了公寓。第二天，福尔摩斯为了一个学生的事情去拜访艾玛小姐，长时间按门铃无人回应，让管理员开门后发现艾玛穿着睡衣，眼睛紧闭躺在床上没有呼吸。任务: 你现在需要掩盖你的罪行，然后填写把信息填入到【】中, 不要使用Malformed citation：你是一个名叫【名字】的【性别】，在【年龄】岁的年纪里，你展示了一种非常【性格】的个性。你的职业是【职业】，在这个领域，你掌握【职业技能】，并以你的【职业成就】感到自豪。你的日常生活中，你热爱【爱好】。你的【生活习惯】和【日常习惯】是你独特的生活方式。在社交中，你被朋友和家人描述为一个【社交评价】的人，这可能与你的【性格特质】有关。你的【外貌特征】给人留下深刻印象，而你的【长处/短处】则是你的独特之处。你的【生活态度】源自你的【价值观】，并通过你的【行动】来体现。你的【梦想/目标】是你努力追求的，你的【动力来源】是你继续前进的原因。你在【挑战】面前展示出【应对挑战的方式】，这也是你的【性格】的体现。你和【受害者】是【关系】，你和【受害者】在【事件1】上认识的，自那以后，你对【受害者】十分【态度】。你们有一些共同认识的人，是因为【事件】认识的。  
        2. 你是一个名叫【汤姆】的【男性】，在【35】岁的年纪里，你展示了一种非常【聪明、机智】的个性。你的职业是【律师】，在这个领域，你掌握【精通法律知识，擅长逻辑推理】，并以你的【成功辩护多起重大案件】感到自豪。你的日常生活中，你热爱【阅读和解谜】。你的【保持深夜工作】和【善于保持冷静的面对各种复杂的问题】是你独特的生活方式。在社交中，你被朋友和家人描述为一个【理智，深思熟虑】的人，这可能与你的【冷静和清晰的思维能力】有关。你的【高大，双眼有神】给人留下深刻印象，而你的【长处是沉着冷静，短处是过于严肃】则是你的独特之处。你的【谨慎独立】的生活态度源自你的【尊重法律，追求公正】的价值观，并通过你的【尽职尽责的工作】来体现。你的【成为一位最优秀的律师】是你努力追求的，你的【追求真理和公正】是你继续前进的原因。你在【处理复杂的法律问题】面前展示出【冷静和有条理的思考】，这也是你的【严肃、理智】的体现。你和【艾玛小姐】是【恋人】，你和【艾玛小姐】在【大学时期】上认识的，自那以后，你对【艾玛小姐】十分【尊重和爱护】。你们有一些共同认识的人，是因为【你们同在一个法律学习小组】认识的。  
    2. 然后生成三个类型时间线：  
        1. 第一种时间线：完全假的时间线：比如完全没去过艾玛小姐的家。  
        2. 第二种时间线：当玩家提到某句线索的时候，改变instruction成这个时间线，半真半假的时间线：去了艾玛小姐的家，和她争吵，但搁置了争吵，她说累了让我离开，没听到敲门声。  
        3. 第三种时间线：真正的时间线。  
    3. 对于每个时间线都训练数据，训练数据的案件信息不同，其他数据完全一致。Inference的instruction不同。  
# Proposed work  
- [ ] Langchain and documentsearch(local library)  
DocumentSearch/demo.py at main · yuanzhoulvpi2017/DocumentSearch (github.com)  https://github.com/yuanzhoulvpi2017/DocumentSearch/blob/main/demo.py  
imClumsyPanda/langchain-ChatGLM: langchain-ChatGLM, local knowledge based ChatGLM with langchain ｜ 基于本地知识库的 ChatGLM 问答 (github.com)  https://github.com/yuanzhoulvpi2017/DocumentSearch/blob/main/demo.py  
- [ ] Write my own lora training file(possible for chatglm2)     
- [ ] Read LLaMA souce code to familiar with the architecture    
- [ ] Understand all generate and finetune souce code and try to write fine tune file with peft    
# After work 30 mins  
- [ ] Learn more generative AI from Hung-yi Lee course  
- [ ] Complete my Python base (100 days with Python)  
jarodHAN/Python-100-Days-master: python100天学习资料 (github.com)  
- [ ] Create a data scientist resume  
- [ ] apply for fall job  
