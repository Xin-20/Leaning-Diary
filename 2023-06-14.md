# Today's work
- [x] Deploy Linly-Chinese-LLaMA-7B  
python3 /root/linly/Linly/scripts/convert_llama_from_tencentpretrain_to_hf.py     --tp_model_dir Chinese-LLaMA-7B     --input_dir Original_llama_model     --model_size 7B     --tp_model_name chinese_llama_7b     --output_dir linly-Ch-7b  
- [x] test Linly-Chinese-LLaMA-7B  
python inference_LLaMA.py --base_model linly-Ch-7b --load_in_8bit True  
- [x] Read through zhuxian  
- [x] Finetune Linly-7B with alpaca4zh.json  
python finetune.py     --base_model 'linly-Ch-7b'     --data_path 'alpaca4zh.json'     --output_dir './fintune-Linly-7b'  

# Questions
1. The end of the 7b linly seems to be problematic, but the bos, eos, pad token from config are exact same as we set in inference file.  
    1. try to fine tune it first. Maybe it could be better then.  
2. What is the difference between the instruction tuning and the in-context learning. Our fine tune on the LLama model is a instruction tuning or in-context learning?  
    1. My guest and understanding now: instruction is about what task you need to do, and context is the example of the task, instruction tuning means if you give some training example on the instruction, then next time it see the instruction, the model would know what to do next. However, the in-context learning does not have an instruction, it   

# Gossip

# Proposed work
- [ ] Fine tune 7b or ziya model with the zhuxian data set  
- [ ] My inference should take input into account. The ture format should be:  
'### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n  '  
'### Instruction:\n{instruction}\n\n### Response:\n  '  
rather than just take:  
'### Instruction:\n{instruction}\n\n### Response:\n all the time.'  

# After work 30 mins
- [x] Hung-yi Lee courses
ML 2023 Spring (ntu.edu.tw)  https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php  
- [ ] Pipelines for inference (huggingface.co)  https://huggingface.co/docs/transformers/v4.30.0/en/pipeline_tutorial  
- [ ] Last week leetcode contest q4  
- [x] After 9:30, Enroll NLP courses  
- [ ] Create a data scientist resume  
- [ ] apply for fall job  
