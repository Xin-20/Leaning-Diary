# Today's work
- [x] Daily conversation data generated and correct.  
- [x] Use Ziya 13B to finetune on basic dataset  
python finetune.py --base_model 'Ziya-LLaMA-13B/' --data_path 'data/up-2023-7-5-doc2-merged.json' --output_dir './finetune-Ziya-up-1' --num_epochs 30 --learning_rate 1e-4 --cutoff_len 1024 --val_set_size 0 --lora_target_modules '[q_proj,v_proj,k_proj,o_proj]' --add_eos_token True  
- [x] test Ziya 13B  
python generate.py     --load_8bit     --base_model 'Ziya-LLaMA-13B/'     --lora_weights 'finetune-Ziya-detective'  
- [x] Add family negative sample, and test Ziya 13B to compare the difference  
- [ ] Baichuan model -> 32K  
arxiv.org/pdf/2104.09864.pdf  https://arxiv.org/pdf/2104.09864.pdf  
2306.15595.pdf (arxiv.org)  https://arxiv.org/pdf/2306.15595.pdf
# Questions  
# Gossip  
# Proposed work 
- [ ] Read PEFT Repo, typing, dataclasses library    
- [ ] Read finetune and inference code    
- [ ] Read achitecture source code      
- [ ] Langchain and documentsearch(local library)  
DocumentSearch/demo.py at main · yuanzhoulvpi2017/DocumentSearch (github.com)  https://github.com/yuanzhoulvpi2017/DocumentSearch/blob/main/demo.py  
imClumsyPanda/langchain-ChatGLM: langchain-ChatGLM, local knowledge based ChatGLM with langchain ｜ 基于本地知识库的 ChatGLM 问答 (github.com)  https://github.com/imClumsyPanda/langchain-ChatGLM  
- [ ] Write my own lora training file(possible for chatglm2)  
- [ ] Understand all generate and finetune souce code and try to write fine tune file with peft  
# After work 30 mins
- [ ] Learn more generative AI from Hung-yi Lee course  
- [ ] Complete my Python base (100 days with Python)  
jarodHAN/Python-100-Days-master: python100天学习资料 (github.com)  https://github.com/jarodHAN/Python-100-Days-master  
- [ ] Create a data scientist resume  
- [ ] apply for fall job  
