# Today's work
- [x] Chatglm2 finetune  
CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \  
    --do_train \  
    --dataset dialog_all \  
    --finetuning_type lora \  
    --output_dir detective-1 \  
    --per_device_train_batch_size 4 \  
    --gradient_accumulation_steps 4 \  
    --lr_scheduler_type cosine \  
    --logging_steps 10 \  
    --save_steps 1000 \  
    --learning_rate 5e-5 \  
    --num_train_epochs 100 \  
    --fp16\  
    --use_v2  
ChatGLM-Efficient-Tuning/README_zh.md at main · hiyouga/ChatGLM-Efficient-Tuning · GitHub  https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md  
PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU - YouTube  https://www.youtube.com/watch?v=YVU5wAA6Txo  
- [x] finetune glm2 output  
python src/export_model.py \  
    --checkpoint_dir detective-1/ \  
    --output_dir detective-1-model  
- [x] glm2 inference  
In ChatGLM2-6B python web_demo.py  
In ChatGLM-Efficient-Tuning python src/web_demo.py     --checkpoint_dir detective-1  
- [x] check how the ChatGLM-Efficient-Tuning handle the instruction and input  
Detective-1: only tom  
Detective-2: tom and sherley 50 epoches  
Detective-3: tom and sherley 100 epoches  
Detective-4: tom and sherley 20 epoches  
Detective-5: tom and sherley (add relationship like family and AI negative sample) 30 epoches  
# Questions
1. Cannot train the model, maybe because the model path is not right.  
    1. change the model path in config.py  
# Gossip  
1. What is Lora and PEFT?  
    1. Lora is one of the adapter that adopt SVD, make the original matrix  
# Proposed work
- [ ] Finetune baichuan
hiyouga/LLaMA-Efficient-Tuning: Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA) (github.com)  https://github.com/hiyouga/LLaMA-Efficient-Tuning  
- [ ] Read PEFT Repo, typing, dataclasses library  
- [ ] Read finetune and inference code  
- [ ] Read achitecture source code  
- [ ] Langchain and documentsearch(local library)  
DocumentSearch/demo.py at main · yuanzhoulvpi2017/DocumentSearch (github.com)  https://github.com/yuanzhoulvpi2017/DocumentSearch/blob/main/demo.py  
imClumsyPanda/langchain-ChatGLM: langchain-ChatGLM, local knowledge based ChatGLM with langchain ｜ 基于本地知识库的 ChatGLM 问答 (github.com)  https://github.com/imClumsyPanda/langchain-ChatGLM  
- [ ] Write my own lora training file(possible for chatglm2)  
- [ ] Understand all generate and finetune souce code and try to write fine tune file with peft  
# After work 30 mins
- [ ] Learn more generative AI from Hung-yi Lee course  
- [ ] Complete my Python base (100 days with Python)  
jarodHAN/Python-100-Days-master: python100天学习资料 (github.com)  https://github.com/jarodHAN/Python-100-Days-master  
- [ ] Create a data scientist resume  
- [ ] apply for fall job  

