# Today's work
- [x] Compare the model with and without train_on_input  
python finetune_baichuan.py --base_model 'Baichuan-13B-Chat' --data_path 'data/mishi-07-21.json' --output_dir './Finetune-model/finetune-baichuan-up-4' --num_epochs 5  --learning_rate 1e-4 --cutoff_len 1024 --val_set_size 0  --add_eos_token True --train_on_inputs False  
- [x] Reset my daily conversation data instruction(delete the workflow)  
- [x] Train the model with daily conversation data  
python finetune_baichuan.py --base_model 'Baichuan-13B-Chat' --data_path 'data/up-2023-07-27-daily.json' --output_dir './Finetune-model/finetune-baichuan-up-5' --num_epochs 10  --learning_rate 1e-4 --cutoff_len 1024 --val_set_size 0  --add_eos_token True --train_on_inputs False  
- [x] handle the mishi json data into a matchable list  
- [x] test the match  
- [x] append the match to the prompt  
- [ ] test the result  
python generate_baichuan_match.py --base_model "Baichuan-13B-Chat/" --lora_weights "Finetune-model/finetune-baichuan-up-5"  
# Questions  
# Gossip
1. Why we use dataclass decorator: 理解 Python 的 Dataclasses（一） - 知乎 (zhihu.com)  https://zhuanlan.zhihu.com/p/59657729  
2. template: jinja2: Jinja2 入门教程、基本概念、简单使用及使用 Jinja2 生成 H3C 交换机配置举例_某呆啊的博客-CSDN博客  https://blog.csdn.net/q965844841qq/article/details/105532428  
3. Becasue I only got limited data and computing power, I cannot full tuning the model on a large roleplay dataset to get a chat model and then lora tuning on it. So my idea right now is, I have 3 types of ways:  
    1. use the daily conversation data to teaching the chat model how to speak as I wish(lora tuning) to a low loss, then use langchain to match the possible question and answers for mishi. The instruction is: the background setting + mishi num. The matched sentence: mishi num + input + output  
    2. use the daily conversation data and the mishi data to teaching the chat model how to speak as I wish(lora tuning). The instruction is: the background setting + mishi detail.  
    3. try the first two methods on the basic model.  
    4. generate more roleplay data for enhance the model roleplay capbility.  
# Proposed work
# After work
- [ ] Linux Learning
- [ ] Create a data scientist resume
- [ ] apply for fall job
